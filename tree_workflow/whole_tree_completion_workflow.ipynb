{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7ecec78",
   "metadata": {},
   "source": [
    "Apply point cloud completion (based on PoinTr) to a xyz cloud of any size. Most suitable for single trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "903e60c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "#import open3d as o3d\n",
    "import tree2cubes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d294508",
   "metadata": {},
   "source": [
    "### Load point cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caeb2a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to incomplete point cloud\n",
    "infile = \"/Stor1/wout/OcclusionPaper/data_treepointr_test/input/ABI_ground_1cm_SOR_6_10.txt\"\n",
    "treename = \"ABI\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce4d23eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load incomplete pointcloud\n",
    "point_cloud = np.loadtxt(infile, skiprows=0, delimiter=\",\") # delimiter=\",\"\n",
    "\n",
    "# load .ply file and convert to numpy array\n",
    "# ply_cloud = o3d.io.read_point_cloud(item)\n",
    "# point_cloud = np.asarray(ply_cloud.points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb78a4c",
   "metadata": {},
   "source": [
    "### Cut into samples\n",
    "\n",
    "PoinTr only allows input point clouds of a limited size. The treePoinTr models were trained on point cloud samples of 1m^3  containing between 2730 and 8192 points.\n",
    "To apply completion on entire trees or even plots, larger point clouds need to be cut into cubes (voxels) to perform inference.\n",
    "We use the function cut_point_cloud() to voxelize the point cloud four times with spatially shifted grids and specifyable voxel sizes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25853d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cut the point cloud into cubes and save as .txt files. Choose 4 cube sizes approx. between 0.6 and 1.8 m\n",
    "outpath = \"/Stor1/wout/OcclusionPaper/data_treepointr_test/cubes/\"\n",
    "tree2cubes.cut_point_cloud(point_cloud, outpath, size1=1, size2=1, size3=1.25, size4=1.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a3b692",
   "metadata": {},
   "source": [
    "Optional data augmentation step:\n",
    "make addtional versions of the cubes where x and z are switched. \n",
    "(inference results are sometimes rotation dependent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1172ca0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make versions of the cubes where x and z are switched\n",
    "path=\"/Stor1/wout/OcclusionPaper/data_treepointr_test/cubes/\"\n",
    "for files in glob.glob(path+\"*.txt\"): \n",
    "    data = np.loadtxt(files)\n",
    "    filename = os.path.basename(files)\n",
    "    # Swap the first and third columns\n",
    "    flipfile = np.column_stack((data[:, 2], data[:, 1], data[:, 0]))\n",
    "    np.savetxt(path+filename+\"_flip.txt\", flipfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437bdf45",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "Inference the samples with a pretrained model, following the instructions on https://github.com/yuxumin/PoinTr\n",
    "\n",
    "For example, inference all samples under cubes/ and save the results under inference_result/, using the model real_ckpt-best.pth:\n",
    "\n",
    "\n",
    "python tools/inference.py \\\n",
    "cfgs/real_models/PoinTr.yaml ckpts/real_ckpt-best.pth \\\n",
    "--pc_root cubes/ \\ \n",
    "--save_vis_img  \\\n",
    "--out_pc_root inference_result/ \\\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc7ba16",
   "metadata": {},
   "source": [
    "### Convert and merge predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e7ef6d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotADirectoryError",
     "evalue": "[Errno 20] Not a directory: '/Stor1/wout/OcclusionPaper/data_treepointr_test/output/cube_16_30_32_v2_pred.xyz/fine.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotADirectoryError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m full_pred \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m))\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mdir\u001b[39m \u001b[38;5;129;01min\u001b[39;00m dirs:\n\u001b[0;32m----> 6\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdir\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfine.npy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     np\u001b[38;5;241m.\u001b[39msavetxt(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(pred_path, \u001b[38;5;28mdir\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_pred.xyz\u001b[39m\u001b[38;5;124m\"\u001b[39m), a)\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# cloud = o3d.geometry.PointCloud()\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# cloud.points = o3d.utility.Vector3dVector(a)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# o3d.io.write_point_cloud(pred_path+\"/\"+dirs+\"_pred.ply\", cloud)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/treePoinTr/lib/python3.10/site-packages/numpy/lib/_npyio_impl.py:451\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    449\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 451\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    452\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mNotADirectoryError\u001b[0m: [Errno 20] Not a directory: '/Stor1/wout/OcclusionPaper/data_treepointr_test/output/cube_16_30_32_v2_pred.xyz/fine.npy'"
     ]
    }
   ],
   "source": [
    "# convert all .npy files of predictions into .xyz (or .ply)\n",
    "pred_path=\"/Stor1/wout/OcclusionPaper/data_treepointr_test/output/\"\n",
    "dirs = os.listdir(path=pred_path)\n",
    "full_pred = np.empty((2, 3))\n",
    "for dir in dirs:\n",
    "    a = np.load(os.path.join(pred_path, dir, \"fine.npy\"))\n",
    "    np.savetxt(os.path.join(pred_path, dir, \"_pred.xyz\"), a)\n",
    "    # cloud = o3d.geometry.PointCloud()\n",
    "    # cloud.points = o3d.utility.Vector3dVector(a)\n",
    "    # o3d.io.write_point_cloud(pred_path+\"/\"+dirs+\"_pred.ply\", cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f7d056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# reverse the switch of x and z on predictions if necessary\n",
    "# and merge all predictions into one cloud  \n",
    "\n",
    "full_pred = np.empty((2, 3))\n",
    "pred1 = np.empty((2, 3))\n",
    "predflip = np.empty((2, 3))\n",
    "\n",
    "\n",
    "for files in glob.glob(pred_path+\"*.xyz\"): \n",
    "    data = np.loadtxt(files)\n",
    "    filename = os.path.basename(files)\n",
    "    newfile = np.column_stack((data[:, 2], data[:, 1], data[:, 0]))\n",
    "    # Swap the first and third columns\n",
    "    if \"flip\" in filename:\n",
    "        #print(\"found flip\")\n",
    "        predflip = np.concatenate((predflip, newfile), 0)   \n",
    "    else:\n",
    "        newfile = data\n",
    "        pred1 = np.concatenate((pred1, newfile), 0)\n",
    "   \n",
    "\n",
    "np.savetxt(pred_path+f\"/{treename}_completion.xyz\", pred1)\n",
    "np.savetxt(pred_path+f\"/{treename}_completion_withflips.xyz\", predflip)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13aae4e8",
   "metadata": {},
   "source": [
    "### Post-processing\n",
    "\n",
    "Ideally, the completed point clouds are now filtered in CloudCompare, using e.g. SOR filter and Gemetric features (Surface density)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "treePoinTr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
